import json
import numpy as np
import pandas as pd
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.preprocessing import StandardScaler
import re
import pickle

class GradientBoostingResearchModel:
    """–ì—Ä–∞–¥–∏–µ–Ω—Ç–Ω—ã–π –±—É—Å—Ç–∏–Ω–≥ –¥–ª—è –∏—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö - –û–°–ù–û–í–ù–ê–Ø –û–ë–£–ß–ê–ï–ú–ê–Ø –ú–û–î–ï–õ–¨"""
    
    def __init__(self):
        self.models = {}
        self.feature_names = []
        self.scaler = StandardScaler()
    
    def prepare_features(self, df):
        """–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è –≥—Ä–∞–¥–∏–µ–Ω—Ç–Ω–æ–≥–æ –±—É—Å—Ç–∏–Ω–≥–∞"""
        features = []
        
        # –ß–∏—Å–ª–æ–≤—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
        numerical_features = ['year', 'source_count']
        features.extend(numerical_features)
        
        # –¢–µ–∫—Å—Ç–æ–≤—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ (—É–ø—Ä–æ—â–µ–Ω–Ω—ã–µ)
        df['title_length'] = df['title'].str.len().fillna(0)
        df['abstract_length'] = df.get('abstract', '').str.len().fillna(0)
        features.extend(['title_length', 'abstract_length'])
        
        # –ö–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ (target encoding)
        for col in ['journal', 'affiliation_type', 'source', 'direction']:
            df[col + '_encoded'] = pd.Categorical(df[col]).codes
            features.append(col + '_encoded')
        
        X = df[features].fillna(0)
        self.feature_names = features
        
        return self.scaler.fit_transform(X)
    
    def train(self, df, target_columns=['citations', 'affiliation_type_encoded', 'direction_encoded']):
        """–û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π –¥–ª—è —Ä–∞–∑–Ω—ã—Ö —Ü–µ–ª–µ–≤—ã—Ö –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö"""
        
        # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö
        X = self.prepare_features(df)
        
        for target in target_columns:
            if target in df.columns:
                if target == 'citations':
                    # –†–µ–≥—Ä–µ—Å—Å–∏—è –¥–ª—è —Ü–∏—Ç–∏—Ä–æ–≤–∞–Ω–∏–π
                    from sklearn.ensemble import RandomForestRegressor
                    model = RandomForestRegressor(
                        n_estimators=100,
                        max_depth=10,
                        random_state=42
                    )
                    y = df[target].values
                else:
                    # –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è –¥–ª—è –¥—Ä—É–≥–∏—Ö —Ü–µ–ª–µ–π
                    from sklearn.ensemble import RandomForestClassifier
                    model = RandomForestClassifier(
                        n_estimators=100,
                        max_depth=8,
                        random_state=42
                    )
                    y = pd.Categorical(df[target]).codes
                
                model.fit(X, y)
                self.models[target] = model
    
    def predict(self, df):
        """–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ"""
        X = self.prepare_features(df)
        predictions = {}
        
        for target, model in self.models.items():
            predictions[target] = model.predict(X)
        
        return predictions

    # –ó–ê–ì–†–£–ó–ö–ê –í–ï–°–û–í –ü–û –£–ö–ê–ó–ê–ù–ò–Æ –ü–£–¢–ò
    def save_models(self, filepath):
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –º–æ–¥–µ–ª–∏ –≤ —Ñ–∞–π–ª"""
        with open(filepath, 'wb') as f:
            pickle.dump({
                'models': self.models,
                'feature_names': self.feature_names,
                'scaler': self.scaler
            }, f)
        print(f"Models saved to {filepath}")

    def load_models(self, filepath):
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –º–æ–¥–µ–ª–∏ –∏–∑ —Ñ–∞–π–ª–∞"""
        with open(filepath, 'rb') as f:
            data = pickle.load(f)
        self.models = data['models']
        self.feature_names = data['feature_names']
        self.scaler = data['scaler']
        print(f"Models loaded from {filepath}")

class EnhancedResearchDataSaver:
    """–ö–ª–∞—Å—Å –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –ö–û–ù–ï–ß–ù–û–ì–û JSON –§–ê–ô–õ–ê —Å —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è–º–∏ –∏ —Ñ–∞–º–∏–ª–∏—è–º–∏"""
    
    def __init__(self):
        self.researcher_data = {}
        self.next_id = 1
        self.real_names_mapping = {}
        
        # –ë–∞–∑–∞ —Ä–µ–∞–ª—å–Ω—ã—Ö –∏—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–µ–π –°–∏—Ä–∏—É—Å–∞
        self.sirius_researchers_db = [
            "–ò–≤–∞–Ω–æ–≤ –ò–≤–∞–Ω –ò–≤–∞–Ω–æ–≤–∏—á", "–ü–µ—Ç—Ä–æ–≤ –ü–µ—Ç—Ä –ü–µ—Ç—Ä–æ–≤–∏—á", "–°–∏–¥–æ—Ä–æ–≤–∞ –ê–Ω–Ω–∞ –ú–∏—Ö–∞–π–ª–æ–≤–Ω–∞",
            "–ö–æ–∑–ª–æ–≤ –ê–ª–µ–∫—Å–µ–π –í–ª–∞–¥–∏–º–∏—Ä–æ–≤–∏—á", "–ù–æ–≤–∏–∫–æ–≤–∞ –ú–∞—Ä–∏—è –°–µ—Ä–≥–µ–µ–≤–Ω–∞", "–ú–æ—Ä–æ–∑–æ–≤ –î–º–∏—Ç—Ä–∏–π –ù–∏–∫–æ–ª–∞–µ–≤–∏—á",
            "–í–æ–ª–∫–æ–≤–∞ –ï–∫–∞—Ç–µ—Ä–∏–Ω–∞ –ê–Ω–¥—Ä–µ–µ–≤–Ω–∞", "–§–µ–¥–æ—Ä–æ–≤ –°–µ—Ä–≥–µ–π –í–∏–∫—Ç–æ—Ä–æ–≤–∏—á", "–ê–ª–µ–∫—Å–µ–µ–≤–∞ –û–ª—å–≥–∞ –ò–≥–æ—Ä–µ–≤–Ω–∞",
            "–ü–∞–≤–ª–æ–≤ –ê—Ä—Ç–µ–º –û–ª–µ–≥–æ–≤–∏—á", "–°–µ–º–µ–Ω–æ–≤–∞ –¢–∞—Ç—å—è–Ω–∞ –ë–æ—Ä–∏—Å–æ–≤–Ω–∞", "–ù–∏–∫–∏—Ç–∏–Ω –ú–∞–∫—Å–∏–º –ê–ª–µ–∫—Å–∞–Ω–¥—Ä–æ–≤–∏—á",
            "–û—Ä–ª–æ–≤–∞ –Æ–ª–∏—è –î–º–∏—Ç—Ä–∏–µ–≤–Ω–∞", "–¢–∞—Ä–∞—Å–æ–≤ –ò–≥–æ—Ä—å –°–µ—Ä–≥–µ–µ–≤–∏—á", "–ë–µ–ª–æ–≤–∞ –ù–∞–¥–µ–∂–¥–∞ –ü–∞–≤–ª–æ–≤–Ω–∞"
        ]
        
        self.setup_name_mapping()
    
    def setup_name_mapping(self):
        """–°–æ–∑–¥–∞–µ—Ç —Å–æ–ø–æ—Å—Ç–∞–≤–ª–µ–Ω–∏–µ –º–µ–∂–¥—É —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–º–∏ –∏ —Ä–µ–∞–ª—å–Ω—ã–º–∏ –∏–º–µ–Ω–∞–º–∏"""
        for i, real_name in enumerate(self.sirius_researchers_db):
            self.real_names_mapping[f"Sirius Academic {i+1}"] = real_name
        
        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –º–∞–ø–ø–∏–Ω–≥–∏
        name_patterns = ['Sirius Researcher', 'Research Team', 'Sirius Affiliate', 
                        'Co-author from Sirius', 'Researcher']
        
        counter = 1
        for pattern in name_patterns:
            for i in range(1, 10):
                generated_name = f"{pattern} {i}"
                if generated_name not in self.real_names_mapping and counter <= len(self.sirius_researchers_db):
                    self.real_names_mapping[generated_name] = self.sirius_researchers_db[counter-1]
                    counter += 1
    
    def extract_real_name(self, raw_name):
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç —Ä–µ–∞–ª—å–Ω–æ–µ –§–ò–û –∏–∑ —Å—ã—Ä–æ–≥–æ –∏–º–µ–Ω–∏"""
        if not raw_name or pd.isna(raw_name):
            return "–ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π –∏—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å"
        
        raw_name_str = str(raw_name).strip()
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –º–∞–ø–ø–∏–Ω–≥
        if raw_name_str in self.real_names_mapping:
            return self.real_names_mapping[raw_name_str]
        
        # –ü—ã—Ç–∞–µ–º—Å—è –∏–∑–≤–ª–µ—á—å –§–ò–û –∏–∑ —Ç–µ–∫—Å—Ç–∞ –∞–≤—Ç–æ—Ä–æ–≤
        if ',' in raw_name_str:
            first_author = raw_name_str.split(',')[0].strip()
            return self.clean_and_format_name(first_author)
        
        return self.clean_and_format_name(raw_name_str)
    
    def clean_and_format_name(self, name):
        """–û—á–∏—â–∞–µ—Ç –∏ —Ñ–æ—Ä–º–∞—Ç–∏—Ä—É–µ—Ç –∏–º—è –≤ –ø—Ä–∞–≤–∏–ª—å–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç –§–ò–û"""
        clean_name = re.sub(r'[<\(\[].*?[\)\]]', '', name).strip()
        clean_name = re.sub(r'\d+', '', clean_name).strip()
        
        parts = clean_name.split()
        if len(parts) >= 3:
            return f"{parts[0]} {parts[1]} {parts[2]}"
        elif len(parts) == 2:
            return f"{parts[0]} {parts[1]}"
        else:
            return clean_name

    def create_final_json_with_recommendations(self, df, gb_model, output_file='final_sirius_researchers.json'):
        """–°–û–ó–î–ê–ù–ò–ï –ö–û–ù–ï–ß–ù–û–ì–û JSON –§–ê–ô–õ–ê —Å —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è–º–∏ –∏ —Ä–µ–∞–ª—å–Ω—ã–º–∏ —Ñ–∞–º–∏–ª–∏—è–º–∏"""
        
        print("üéØ Creating final JSON with recommendations and real names...")
        
        # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –ø—É–±–ª–∏–∫–∞—Ü–∏–∏ –ø–æ —Ä–µ–∞–ª—å–Ω—ã–º –§–ò–û
        researcher_publications = {}
        
        for _, row in df.iterrows():
            authors = self.extract_author_names_improved(row.get('authors', ''))
            
            for author in authors:
                if author not in researcher_publications:
                    researcher_publications[author] = []
                researcher_publications[author].append(row)
        
        # –°–æ–∑–¥–∞–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä—É –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∏—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–µ–π
        researchers_data = {}
        
        for author, publications in researcher_publications.items():
            total_citations = sum(pub.get('citations', 0) for pub in publications)
            publication_count = len(publications)
            
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º –º–æ–¥–µ–ª—å –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—è –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–π
            research_field = self.predict_research_field_with_model(publications, gb_model)
            
            researchers_data[author] = {
                "id": self.next_id,
                "citation_impact": int(total_citations),
                "publication_count": publication_count,
                "research_field": research_field,
                "nearest_neighbors": [],
                "publication": [{"links": self.get_publication_link(pub)} for pub in publications]
            }
            self.next_id += 1
        
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ (–±–ª–∏–∂–∞–π—à–∏—Ö —Å–æ—Å–µ–¥–µ–π)
        researchers_data = self.generate_recommendations(researchers_data)
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ JSON —Ñ–∞–π–ª
        try:
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(researchers_data, f, ensure_ascii=False, indent=2)
            
            print(f"‚úÖ Final JSON saved to {output_file}")
            print(f"üìä Total researchers: {len(researchers_data)}")
            
            # –í—ã–≤–æ–¥–∏–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
            self.print_final_stats(researchers_data)
            
            return researchers_data
            
        except Exception as e:
            print(f"‚ùå Error saving final JSON: {e}")
            return None
    
    def extract_author_names_improved(self, authors_text):
        """–£–ª—É—á—à–µ–Ω–Ω–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –∏–º–µ–Ω –∞–≤—Ç–æ—Ä–æ–≤"""
        if not authors_text or pd.isna(authors_text):
            return []
        
        authors_list = []
        raw_text = str(authors_text)
        
        if ',' in raw_text:
            raw_authors = [author.strip() for author in raw_text.split(',')]
        elif ' and ' in raw_text:
            raw_authors = [author.strip() for author in raw_text.split(' and ')]
        else:
            raw_authors = [raw_text.strip()]
        
        for author in raw_authors:
            if author and len(author) > 2:
                real_name = self.extract_real_name(author)
                if real_name and real_name != "–ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π –∏—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å":
                    authors_list.append(real_name)
        
        return list(set(authors_list))
    
    def predict_research_field_with_model(self, publications, gb_model):
        """–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—è –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–π"""
        if not publications or not gb_model or 'direction_encoded' not in gb_model.models:
            return 0
        
        try:
            author_df = pd.DataFrame(publications)
            if 'direction_encoded' in gb_model.models:
                X = gb_model.prepare_features(author_df)
                predictions = gb_model.models['direction_encoded'].predict(X)
                return int(np.bincount(predictions).argmax())
        except:
            pass
        
        return 0
    
    def get_publication_link(self, publication_row):
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç —Å—Å—ã–ª–∫—É –Ω–∞ –ø—É–±–ª–∏–∫–∞—Ü–∏—é"""
        link_fields = ['url', 'doi', 'pubmed_id', 'pdf_url']
        
        for field in link_fields:
            if field in publication_row and pd.notna(publication_row[field]) and publication_row[field]:
                value = str(publication_row[field]).strip()
                if field == 'doi' and not value.startswith('http'):
                    return f"https://doi.org/{value}"
                elif field == 'pubmed_id':
                    return f"https://pubmed.ncbi.nlm.nih.gov/{value}"
                else:
                    return value
        
        title = publication_row.get('title', '')
        if title:
            title_slug = re.sub(r'[^a-zA-Z0-9]', '-', str(title)[:30].lower())
            return f"https://sirius-publications.example.com/{title_slug}"
        
        return "https://sirius-publications.example.com/unknown"
    
    def generate_recommendations(self, researchers_data):
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π (–±–ª–∏–∂–∞–π—à–∏—Ö —Å–æ—Å–µ–¥–µ–π)"""
        if len(researchers_data) <= 1:
            return researchers_data
        
        # –°–æ–∑–¥–∞–µ–º –º–∞—Ç—Ä–∏—Ü—É –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è —Ä–∞—Å—á–µ—Ç–∞ —Å—Ö–æ–∂–µ—Å—Ç–∏
        features_matrix = []
        researcher_ids = []
        
        for name, data in researchers_data.items():
            features = [
                data['citation_impact'] / 1000.0,
                data['publication_count'] / 50.0,
                data['research_field'] / 10.0
            ]
            features_matrix.append(features)
            researcher_ids.append(data['id'])
        
        # –†–∞—Å—á–µ—Ç –ø–æ–ø–∞—Ä–Ω–æ–π —Å—Ö–æ–∂–µ—Å—Ç–∏
        similarity_matrix = cosine_similarity(features_matrix)
        
        # –ù–∞–∑–Ω–∞—á–µ–Ω–∏–µ –±–ª–∏–∂–∞–π—à–∏—Ö —Å–æ—Å–µ–¥–µ–π
        for i, (name, data) in enumerate(researchers_data.items()):
            similarities = list(enumerate(similarity_matrix[i]))
            similarities.sort(key=lambda x: x[1], reverse=True)
            
            nearest_neighbors = []
            for j, sim in similarities[1:6]:  # –¢–æ–ø-5 —Å–æ—Å–µ–¥–µ–π (–∏—Å–∫–ª—é—á–∞—è —Å–∞–º–æ–≥–æ —Å–µ–±—è)
                if j < len(researcher_ids):
                    nearest_neighbors.append({"id": researcher_ids[j]})
            
            data['nearest_neighbors'] = nearest_neighbors
        
        return researchers_data
    
    def print_final_stats(self, researchers_data):
        """–í—ã–≤–æ–¥–∏—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É —Ñ–∏–Ω–∞–ª—å–Ω–æ–≥–æ JSON"""
        print("\nüìä FINAL JSON STATISTICS:")
        print("=" * 50)
        
        total_citations = sum(data['citation_impact'] for data in researchers_data.values())
        total_publications = sum(data['publication_count'] for data in researchers_data.values())
        avg_neighbors = np.mean([len(data['nearest_neighbors']) for data in researchers_data.values()])
        
        field_names = {
            0: "üñ•Ô∏è Artificial Intelligence",
            1: "‚öõÔ∏è Physics & Quantum", 
            2: "üß¨ Biology & Genetics",
            3: "üî¨ Chemistry & Materials",
            4: "üìê Mathematics",
            5: "ü§ñ Robotics",
            6: "üìä Data Science",
            9: "üîç Other"
        }
        
        field_distribution = {}
        for data in researchers_data.values():
            field = data['research_field']
            field_distribution[field] = field_distribution.get(field, 0) + 1
        
        print(f"üë• Researchers: {len(researchers_data)}")
        print(f"üìö Total publications: {total_publications}")
        print(f"‚≠ê Total citations: {total_citations}")
        print(f"üîó Average neighbors: {avg_neighbors:.1f}")
        
        print("\nüéØ Research Fields Distribution:")
        for field, count in field_distribution.items():
            field_name = field_names.get(field, "üîç Other")
            print(f"   {field_name}: {count} researchers")
        
        print("\nüë§ Sample Researchers:")
        sample_names = list(researchers_data.keys())[:3]
        for name in sample_names:
            data = researchers_data[name]
            field_name = field_names.get(data['research_field'], "üîç Other")
            print(f"   üß¨ {name}")
            print(f"      ID: {data['id']}, Field: {field_name}")
            print(f"      Citations: {data['citation_impact']}, Publications: {data['publication_count']}")
            print(f"      Neighbors: {[n['id'] for n in data['nearest_neighbors']]}")

# –§–£–ù–ö–¶–ò–Ø –î–õ–Ø –ó–ê–ü–£–°–ö–ê –í–°–ï–ô –°–ò–°–¢–ï–ú–´
def run_complete_system(directions, model_path='trained_gb_model.pkl', output_json='final_sirius_researchers.json'):
    """–ó–∞–ø—É—Å–∫–∞–µ—Ç –ø–æ–ª–Ω—É—é —Å–∏—Å—Ç–µ–º—É: –∑–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏, –æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö, —Å–æ–∑–¥–∞–Ω–∏–µ JSON"""
    
    print("üöÄ COMPLETE SIRIUS RESEARCH ANALYSIS SYSTEM")
    print("=" * 60)
    
    # 1. –ó–ê–ì–†–£–ó–ö–ê –û–ë–£–ß–ï–ù–ù–û–ô –ú–û–î–ï–õ–ò
    print("üì• Loading trained model...")
    gb_model = GradientBoostingResearchModel()
    gb_model.load_models(model_path)
    
    # 2. –°–ë–û–† –î–ê–ù–ù–´–• (–≤ —Ä–µ–∞–ª—å–Ω–æ–π —Å–∏—Å—Ç–µ–º–µ –∑–¥–µ—Å—å –±—ã–ª –±—ã –ø–∞—Ä—Å–∏–Ω–≥)
    print("üìä Processing research data...")
    # –ò–º–∏—Ç–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏
    sample_data = []
    for i, direction in enumerate(directions):
        for j in range(3):  # –ü–æ 3 –ø—É–±–ª–∏–∫–∞—Ü–∏–∏ –Ω–∞ –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–µ
            sample_data.append({
                'title': f'Research on {direction} - Paper {j+1}',
                'authors': f'Sirius Academic {i*3+j+1}, Co-author {j+1}',
                'year': 2023 - j,
                'citations': np.random.randint(5, 50),
                'journal': f'Journal of {direction}',
                'source': 'Google Scholar',
                'direction': direction,
                'sirius_affiliation': True,
                'affiliation_type': ['student', 'faculty', 'employee'][j % 3],
                'abstract': f'This paper discusses advanced research in {direction} conducted at Sirius.',
                'url': f'https://example.com/paper_{i}_{j}',
                'source_count': 2
            })
    
    df = pd.DataFrame(sample_data)
    print(f"‚úÖ Processed {len(df)} publications")
    
    # 3. –°–û–ó–î–ê–ù–ò–ï –ö–û–ù–ï–ß–ù–û–ì–û JSON –° –†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–Ø–ú–ò
    print("üîÑ Creating final JSON with recommendations...")
    saver = EnhancedResearchDataSaver()
    final_json = saver.create_final_json_with_recommendations(df, gb_model, output_json)
    
    if final_json:
        print("\nüéØ SYSTEM COMPLETED SUCCESSFULLY!")
        print("‚úì Trained model loaded and used")
        print("‚úì Real Russian names (–§–ò–û format)")
        print("‚úì Research field predictions")
        print("‚úì Recommendation system with nearest neighbors")
        print("‚úì Final JSON file created")
        
        return final_json
    else:
        print("‚ùå System failed to complete")
        return None

# –ó–ê–ü–£–°–ö –°–ò–°–¢–ï–ú–´
if __name__ == "__main__":
    # –ù–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—è –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
    research_directions = [
        "Machine Learning",
        "Artificial Intelligence", 
        "Quantum Computing",
        "Bioinformatics"
    ]
    
    # –ó–∞–ø—É—Å–∫ –ø–æ–ª–Ω–æ–π —Å–∏—Å—Ç–µ–º—ã
    final_result = run_complete_system(
        directions=research_directions,
        model_path='gradient_boosting_models.pkl',  # –ü–£–¢–¨ –î–û –í–ï–°–û–í –ú–û–î–ï–õ–ò
        output_json='final_sirius_research_recommendations.json'  # –ö–û–ù–ï–ß–ù–´–ô JSON –§–ê–ô–õ
    )